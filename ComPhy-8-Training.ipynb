{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "ComPhy-8-Training.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monttj/computational-physics/blob/2021/ComPhy_8_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmXVsvByZPeB"
      },
      "source": [
        "We need to mount the google drive in order to use the existing material."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhFLHeNnmgqo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_mDf_1kmtrg"
      },
      "source": [
        "cd /content/drive/My Drive/Class/2021/bigdata/computational-physics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXkt7roPYhLS"
      },
      "source": [
        "## Training\n",
        "\n",
        "We need to understand backpropagation first. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crAJUBgMYhLY"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpGIwZ1YhLY"
      },
      "source": [
        "Let's take a look at a very simple backpropagation. \n",
        "$$ z = x\\times y $$ \n",
        "\n",
        "We would like to calculate \n",
        "$$dz/dx$$ \n",
        "or \n",
        "$$dz/dy$$ \n",
        "\n",
        "Let's say $t = x \\times y$.\n",
        "Then,\n",
        "\n",
        "$$dz/dx = dz/dt \\cdot dt/dx = 1 \\cdot dt/dx = y $$\n",
        "$$dz/dy = dz/dt \\cdot dt/dy = 1 \\cdot dt/dy = x $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGQy83kTYhLZ"
      },
      "source": [
        "Another example is \n",
        "$$ z = x + y $$\n",
        "\n",
        "when $t = x + y$,\n",
        "\n",
        "$$dz/dx = dz/dt \\cdot dt/dx = 1 \\cdot 1 = 1 $$\n",
        "$$dz/dy = dz/dt \\cdot dt/dy = 1 \\cdot 1 = 1 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HsgM9VnYhLZ"
      },
      "source": [
        "Now let's implement it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNAt-TCDYhLZ"
      },
      "source": [
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * self.y  # exchange x and y\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "\n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "\n",
        "        return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlDt7NWPYhLa"
      },
      "source": [
        "##### example\n",
        "\n",
        "Can you implement following forward propagation?\n",
        "\n",
        "apple_price = apple $\\times$ apple_num\n",
        "\n",
        "price = apple_price $\\times$ tax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7UsNga-YhLa"
      },
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5WpXjSGYhLa"
      },
      "source": [
        "print(\"price:\", int(price))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmsHbFsmYhLb"
      },
      "source": [
        "##### answer:\n",
        "\n",
        "```\n",
        "('price:', 220)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMC9wYopYhLb"
      },
      "source": [
        "we would like to calculate how much it changes in apple_price, tax and apple_number when the price changes by 1. \n",
        "\n",
        "So it is backward propagation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0yzGzdEYhLb"
      },
      "source": [
        "# backward\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dTax:\", dtax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwnqS0G5YhLc"
      },
      "source": [
        "##### answer:\n",
        "\n",
        "```\n",
        "('dApple:', 2.2)\n",
        "('dApple_num:', 110)\n",
        "('dTax:', 200)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFKZx7NzYhLc"
      },
      "source": [
        "Following example is a little bit comlicated but it is the same concept. \n",
        "\n",
        "apple_price = apple $\\times$ apple_num\n",
        "\n",
        "orgnage_price = orange $\\times$ orange_num\n",
        "\n",
        "all_price = apple_price + orange_price\n",
        "\n",
        "price = all_price $\\times$ tax\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgn6PoYYYhLc"
      },
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# layer\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
        "#missing one line?\n",
        "#-> \n",
        "\n",
        "print(\"price:\", int(price))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pef-WgWVYhLc"
      },
      "source": [
        "##### answer:\n",
        "\n",
        "```\n",
        "('price:', 715)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Houv3EYhLc"
      },
      "source": [
        "we would like to calculate how much it changes in apple_price, orange_price, apple_number, orange_number and tax when the price changes by 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EO3RhRtYhLd"
      },
      "source": [
        "# backward\n",
        "dprice = 1\n",
        "##missing one line\n",
        "#-> \n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
        "\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dOrange:\", dorange)\n",
        "print(\"dOrange_num:\", int(dorange_num))\n",
        "print(\"dTax:\", dtax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6skAAaYhLd"
      },
      "source": [
        "##### answer:\n",
        "\n",
        "```\n",
        "('dApple:', 2.2)\n",
        "('dApple_num:', 110)\n",
        "('dOrange:', 3.3000000000000003)\n",
        "('dOrange_num:', 165)\n",
        "('dTax:', 650)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNjw06zYhLd"
      },
      "source": [
        "##### ReLU activation function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZtZZgZdYhLd"
      },
      "source": [
        "$$\\begin{align}\n",
        "    y &= x ~(x \\gt 0) \\\\        \n",
        "      &= 0 ~(x \\leq 0) \n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5E8MAHxYhLe"
      },
      "source": [
        "$$\\begin{align}\n",
        "    \\frac{\\partial y}{\\partial x} &= 1 ~(x \\gt 0) \\\\        \n",
        "      &= 0 ~(x \\leq 0) \n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hw0ODDbYhLe"
      },
      "source": [
        "backward with <b>ReLU</b> function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLzrfAx_YhLe"
      },
      "source": [
        "if x $\\gt$ 0, \n",
        "\n",
        "$$\\frac{\\partial L}{\\partial y} \\to \\frac{\\partial L}{\\partial y}$$ \n",
        "\n",
        "if x $\\leq$ 0, \n",
        "\n",
        "$$\\frac{\\partial L}{\\partial y} \\to 0 $$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5WlWlU1YhLe"
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.mask = ( x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J02XZrCYhLe"
      },
      "source": [
        "##### Sigmoid activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdmAlZ-6YhLe"
      },
      "source": [
        "$$ y = \\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjBA6wLtYhLf"
      },
      "source": [
        "$$ \\frac{\\partial y}{\\partial x} = y^2 e^{-x} = y (1-y) $$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdJD-gisYhLf"
      },
      "source": [
        "backward with sigmoid function\n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial y} \\to \\frac{\\partial L}{\\partial y} y(1-y) $$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwuppqH7YhLf"
      },
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = 1 / 1(1+np.exp(-x))\n",
        "        self.out = out\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4_oieKFYhLf"
      },
      "source": [
        "### Derivatives\n",
        "\n",
        "$$\\frac{ df(x) }{ dx } = \\displaystyle{\\lim_{h \\to 0}} \\frac{ f(x+h) - f(x) }{ h }$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idRehxm0YhLf"
      },
      "source": [
        "##### numerical gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWvBMg91YhLf"
      },
      "source": [
        "def numerical_diff(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    return (f(x+h) - f(x-h)) / (2*h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vSWaajcYhLg"
      },
      "source": [
        "Let's take derivatives for this function\n",
        "$$y = 0.01x^2+0.1x$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEc7wshfYhLg"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def function_1(x):\n",
        "    return 0.01*x**2 + 0.1*x \n",
        "\n",
        "\n",
        "def tangent_line(f, x):\n",
        "    d = numerical_diff(f, x)\n",
        "    print(d)\n",
        "    y = f(x) - d*x\n",
        "    return lambda t: d*t + y # this is a function of t \n",
        "     \n",
        "x = np.arange(0.0, 20.0, 0.1)\n",
        "y = function_1(x)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "\n",
        "tf = tangent_line(function_1, 5)\n",
        "y2 = tf(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-04MqTN3YhLg"
      },
      "source": [
        "##### Partial derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaztwnOAYhLg"
      },
      "source": [
        "We will calculate partial derivatives for the function\n",
        "$$ f = x^2 + y^2 $$\n",
        "And we will make the tangent lines in 2D. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cNsDD5rYhLg"
      },
      "source": [
        "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "def _numerical_gradient_no_batch(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x) # create a shape\n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        # f(x+h) calculation\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)\n",
        "        \n",
        "        # f(x-h) calculation\n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) \n",
        "        \n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        x[idx] = tmp_val # put the original value back\n",
        "        \n",
        "    return grad\n",
        "\n",
        "\n",
        "def numerical_gradient(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "    if x.ndim == 1:\n",
        "        return np.sum(x**2)\n",
        "    else:\n",
        "        return np.sum(x**2, axis=1)\n",
        "\n",
        "\n",
        "def tangent_line(f, x):\n",
        "    d = numerical_gradient(f, x)\n",
        "    print(d)\n",
        "    y = f(x) - d*x\n",
        "    return lambda t: d*t + y\n",
        "     \n",
        "if __name__ == '__main__':\n",
        "    x0 = np.arange(-2, 2.5, 0.25)\n",
        "    x1 = np.arange(-2, 2.5, 0.25)\n",
        "    X, Y = np.meshgrid(x0, x1)\n",
        "    \n",
        "    X = X.flatten()\n",
        "    Y = Y.flatten()\n",
        "    \n",
        "    grad = numerical_gradient(function_2, np.array([X, Y]) )\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
        "    plt.xlim([-2, 2])\n",
        "    plt.ylim([-2, 2])\n",
        "    plt.xlabel('x0')\n",
        "    plt.ylabel('x1')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.draw()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFqpVo8TYhLh"
      },
      "source": [
        "##### Gradient decent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prO3EcsnYhLh"
      },
      "source": [
        "#We will use the function \"numerical_gradient\" which was created previously \n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x\n",
        "    x_history = []\n",
        "\n",
        "    for i in range(step_num):\n",
        "        x_history.append( x.copy() )\n",
        "\n",
        "        grad = numerical_gradient(f, x)\n",
        "        x -= lr * grad\n",
        "\n",
        "    return x, np.array(x_history)\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj6ZCgRCYhLi"
      },
      "source": [
        "init_x = np.array([-3.0, 4.0])    \n",
        "\n",
        "lr = 0.1\n",
        "step_num = 20\n",
        "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
        "\n",
        "plt.plot( [-5, 5], [0,0], '--b')\n",
        "plt.plot( [0,0], [-5, 5], '--b')\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
        "\n",
        "plt.xlim(-3.5, 3.5)\n",
        "plt.ylim(-4.5, 4.5)\n",
        "plt.xlabel(\"X0\")\n",
        "plt.ylabel(\"X1\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i5d03MPYhLi"
      },
      "source": [
        "##### confirm with numerical gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MVb0NZBYhLi"
      },
      "source": [
        "check the difference between numerical calculation and gradient with analytical method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCL4mfzpYhLi"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import sys, os\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "# for now we will use the existing class for the model. But later we will build our own model. \n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# reading data\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch) ## numerical calculation\n",
        "grad_backprop = network.gradient(x_batch, t_batch) #analytical calculation\n",
        "\n",
        "# take the average in difference\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wkA7iwUYhLi"
      },
      "source": [
        "### Simple network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zoV2hPvYhLj"
      },
      "source": [
        "We will create a very simple network for exercise to understand the shape of the weights\n",
        "\n",
        "We will use existing library for functions to make the example simple. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsX7P4GBYhLj"
      },
      "source": [
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        np.random.seed(0) # we will use the same seed in order to have the same answer always\n",
        "        self.W = np.random.randn(2,3) # random number generator according to normal Gaussian distribution \n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.W)\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        z = self.predict(x)\n",
        "        y = softmax(z)\n",
        "        loss = cross_entropy_error(y, t)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX4Sq9h0YhLj"
      },
      "source": [
        "#create a object of the model \n",
        "net = simpleNet()\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "t = np.array([0, 0, 1])\n",
        "\n",
        "f = lambda w: net.loss(x, t)\n",
        "\n",
        "dW = numerical_gradient(f, net.W)\n",
        "\n",
        "# the derivatives will return the shape of W. \n",
        "print(dW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ8u5V28YhLk"
      },
      "source": [
        "##### answer:\n",
        "```\n",
        "[[ 0.44452826  0.14014461 -0.58467287]\n",
        " [ 0.66679239  0.21021692 -0.87700931]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXZLhPINYhLk"
      },
      "source": [
        "### Build your own model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv_0lTgIYhLk"
      },
      "source": [
        "We will create a class of network with two hidden layers with 2 weight and 2 bias parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p3P4nNUYhLk"
      },
      "source": [
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "import pickle\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        # initialize the weights and bias\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    def predict(self, x):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "    \n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        return y\n",
        "        \n",
        "    # x : input variables, t : true value\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        \n",
        "        return cross_entropy_error(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x : input variable, t : true value\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "        grads = {}\n",
        "        \n",
        "        batch_num = x.shape[0]\n",
        "        \n",
        "        # forward\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        #can you build the second hidden layer and output with softmax function? \n",
        "        #you need to use the weight W2 and b2 in this second hidden layer for a2\n",
        "        #for example, a2=np.dot(z1, W2) + b2 and put it into softmax function for output y\n",
        "        #at least two lines\n",
        "      \n",
        "        \n",
        "        # backward\n",
        "        dy = (y - t) / batch_num\n",
        "        grads['W2'] = np.dot(z1.T, dy)\n",
        "        grads['b2'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        da1 = np.dot(dy, W2.T)\n",
        "        dz1 = sigmoid_grad(a1) * da1\n",
        "        grads['W1'] = np.dot(x.T, dz1)\n",
        "        grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "        return grads\n",
        "    \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7j7CRg_YhLk"
      },
      "source": [
        "Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqt5cZWQYhLk"
      },
      "source": [
        "from dataset.mnist import load_mnist\n",
        "\n",
        "# reading data\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2XJXyX8YhLl"
      },
      "source": [
        "Call the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsOH0WCdYhLl"
      },
      "source": [
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJmduTFfYhLl"
      },
      "source": [
        "Now set hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMPQzYrHYhLl"
      },
      "source": [
        "# hyperparameters\n",
        "iters_num = 10000  # set the number of iterations\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   #  mini batch size\n",
        "learning_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCN2x0T5cJya"
      },
      "source": [
        "Please fix follow lines below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv22AMRqYhLl"
      },
      "source": [
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# how many iterations per epoch \n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # obtain mini batch\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # gradient calculation \n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # update parameters : weight and bias\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'): # we need to update the parameters in network\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # save progress\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    # accuracy ca1uation per epoch\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "#save the model for later use\n",
        "network.save_params(\"simpleTwoLayer.pkl\") \n",
        "\n",
        "# draw a graph for accuracy \n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVxcVI-2YhLl"
      },
      "source": [
        "#### Task\n",
        "\n",
        "Now can you increase the accuracy as much as you can?\n",
        "Let's see who will have the higest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqFJbzpCYhLl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}