{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "ComPhy-10-Overtraining.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monttj/computational-physics/blob/2021/ComPhy-10-Overtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaHeE7YvPbow"
      },
      "source": [
        "Only if you are using colab, you need to mount your drive and go to the directory where the necessary files are located : computational physics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF2GjyhDMYfF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_mDf_1kmtrg"
      },
      "source": [
        "cd /content/drive/My Drive/computational-physics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi0n1PVRMVvT"
      },
      "source": [
        "### Overtraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYWVWoDzMVvY"
      },
      "source": [
        "Overgeneralization is what humans do all too often. For example, you go to a foreign country and the taxi driver rips you off. You might say that all taxi drivers in that country are thieves. \n",
        "\n",
        "Overfitting in statistics is production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably. \n",
        "\n",
        "In machine learning, this is called overtraining.\n",
        "\n",
        "To test, we will check the accuracy for train and test data sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoZlqbKpMVvZ"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# will contrain training data sample to produce overfitting\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay setup =======================\n",
        "weight_decay_lambda = 0 # no weight decay\n",
        "#weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01) # learning rate = 0.01 \n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(2000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train) # calculate accuracy for train data\n",
        "        test_acc = network.accuracy(x_test, t_test) # calculate accuracy for test data\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# make a graph==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bq0mdXxMVva"
      },
      "source": [
        "##### task \n",
        "\n",
        "Can you improve the model above to avoid overtraining? \n",
        "you can think of following ways.\n",
        "1. increase train data samples\n",
        "2. use regularization such as weight decay or dropout (see next example for dropout) \n",
        "3. use less layers or nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGvmUnodMVva"
      },
      "source": [
        "##### weight decay method\n",
        "\n",
        "Regularization means constraining a model to make it simpler and reduce the risk of overfitting.\n",
        "For the weight decay method, we add term $ \\frac{1}{2} \\lambda W^2 $ to the loss function to constrain the weights. Expression is as follows:\n",
        "\n",
        "$$ W = W - \\eta ( \\frac{ \\partial L }{ \\partial W} + \\lambda W) $$\n",
        "\n",
        "$$ W = (1-\\lambda \\eta)W - \\eta \\frac{ \\partial L }{ \\partial W} $$\n",
        "\n",
        "Weights are constrained by $ \\lambda \\eta$ term. $\\lambda$ is hyperparameter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cJCpafqMVva"
      },
      "source": [
        "##### dropout method\n",
        "\n",
        "At every training step, every neuron has a probability ```p``` of being temporarily ***dropped out***\n",
        "It will be entirely ignored during this training step but it may be active during the next step\n",
        "Here ```p``` is called the dropout rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncbmS2ArMVvb"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# will contrain training data sample to produce overfitting\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# this time, we will use dropout. These two lines are for setup for dropout ========================\n",
        "use_dropout = True  #  False if you don't want to use dropout\n",
        "dropout_ratio = 0.2 # probability of being dropped out \n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# make a graph ==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QFUH74sMVvb"
      },
      "source": [
        "With dropout method, did we solve the overtraining problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDqvfaE6MVvb"
      },
      "source": [
        "There is no such a rule how to determine the hyperparameters. \n",
        "We should check many values randomly to find the optimial value.\n",
        "Following code shows how to check all different hyperparameter values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rhxJuYrMVvc"
      },
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.util import shuffle_dataset\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# to speed up, reduce size of train sample\n",
        "x_train = x_train[:500]\n",
        "t_train = t_train[:500]\n",
        "\n",
        "# will use 20% as validation \n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train.shape[0] * validation_rate)\n",
        "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = t_train[validation_num:]\n",
        "\n",
        "\n",
        "def __train(lr, weight_decay, epocs=50):\n",
        "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                            output_size=10, weight_decay_lambda=weight_decay)\n",
        "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
        "                      epochs=epocs, mini_batch_size=100,\n",
        "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
        "    trainer.train()\n",
        "\n",
        "    return trainer.test_acc_list, trainer.train_acc_list\n",
        "\n",
        "\n",
        "# randomly choose hyperparameter values=====================================\n",
        "optimization_trial = 100\n",
        "results_val = {}\n",
        "results_train = {}\n",
        "for _ in range(optimization_trial):\n",
        "    # set the range for weight decay parameter and learning rate==============\n",
        "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "    lr = 10 ** np.random.uniform(-6, -2)\n",
        "    # ================================================\n",
        "\n",
        "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
        "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
        "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
        "    results_val[key] = val_acc_list\n",
        "    results_train[key] = train_acc_list\n",
        "\n",
        "# make plots ========================================================\n",
        "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
        "graph_draw_num = 20\n",
        "col_num = 5\n",
        "row_num = int(np.ceil(graph_draw_num / col_num))\n",
        "i = 0\n",
        "\n",
        "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
        "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
        "\n",
        "    plt.subplot(row_num, col_num, i+1)\n",
        "    plt.title(\"Best-\" + str(i+1))\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    if i % 5: plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    x = np.arange(len(val_acc_list))\n",
        "    plt.plot(x, val_acc_list)\n",
        "    plt.plot(x, results_train[key], \"--\")\n",
        "    i += 1\n",
        "\n",
        "    if i >= graph_draw_num:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZrtfAuaMVvh"
      },
      "source": [
        "Can you say which one is the best model without overtraining?"
      ]
    }
  ]
}